{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2654038,
          "sourceType": "datasetVersion",
          "datasetId": 434238
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Fork of Data Wrangling Assigment",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheebahlangat/Data-scince-and-AI-projects/blob/main/Fork_of_Data_Wrangling_Assigment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "shivamb_netflix_shows_path = kagglehub.dataset_download('shivamb/netflix-shows')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "8TOEEq142t3k"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:55:57.726775Z",
          "iopub.execute_input": "2025-06-10T07:55:57.727079Z",
          "iopub.status.idle": "2025-06-10T07:56:00.109107Z",
          "shell.execute_reply.started": "2025-06-10T07:55:57.727049Z",
          "shell.execute_reply": "2025-06-10T07:56:00.10788Z"
        },
        "id": "draU0nyt2t3q",
        "outputId": "d3426bb9-f5cf-48da-efa5-957fe9c266b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/netflix-shows/netflix_titles.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #Data Science project:Data Wrangling\n",
        "This will demonstrate my walkthrough data wrangling project using python\n",
        "The steps we will go through includes:\n",
        " * Data Discovery\n",
        " * Data Structuring\n",
        "* Data Cleaning\n",
        "* Data Validation\n",
        "* Data Publishing\n",
        "   "
      ],
      "metadata": {
        "id": "vtSay6152t3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Data Discovery"
      ],
      "metadata": {
        "id": "uYhONZMA2t33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the Data to a Pandas DataFrame\n",
        "df=pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\n",
        "#Have a quick overview of the data\n",
        "df.info()\n",
        "# Number of rows and columns\n",
        "print(\"Shape of the dataset (R x C):\", df.shape)\n",
        "# List of all column names\n",
        "print(\"Columns in the dataset:\\n\", df.columns.tolist())\n",
        "# Data types of each column\n",
        "print(\"Data types:\\n\", df.dtypes)\n",
        "# Group and Count of missing (null) values in each column\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
        "#Group and Count the  duplicate rows\n",
        "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
        "# Group and Count of duplicate rows\n",
        "print(\"Number of duplicate rows:\", df.duplicated().sum())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:56:00.111381Z",
          "iopub.execute_input": "2025-06-10T07:56:00.111815Z",
          "iopub.status.idle": "2025-06-10T07:56:00.370666Z",
          "shell.execute_reply.started": "2025-06-10T07:56:00.11179Z",
          "shell.execute_reply": "2025-06-10T07:56:00.369797Z"
        },
        "id": "h8t1SHsk2t35",
        "outputId": "334643ce-0d2c-45b4-c452-c68ec80f36c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8807 entries, 0 to 8806\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   show_id       8807 non-null   object\n 1   type          8807 non-null   object\n 2   title         8807 non-null   object\n 3   director      6173 non-null   object\n 4   cast          7982 non-null   object\n 5   country       7976 non-null   object\n 6   date_added    8797 non-null   object\n 7   release_year  8807 non-null   int64 \n 8   rating        8803 non-null   object\n 9   duration      8804 non-null   object\n 10  listed_in     8807 non-null   object\n 11  description   8807 non-null   object\ndtypes: int64(1), object(11)\nmemory usage: 825.8+ KB\nShape of the dataset (R x C): (8807, 12)\nColumns in the dataset:\n ['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description']\nData types:\n show_id         object\ntype            object\ntitle           object\ndirector        object\ncast            object\ncountry         object\ndate_added      object\nrelease_year     int64\nrating          object\nduration        object\nlisted_in       object\ndescription     object\ndtype: object\nMissing values per column:\n show_id            0\ntype               0\ntitle              0\ndirector        2634\ncast             825\ncountry          831\ndate_added        10\nrelease_year       0\nrating             4\nduration           3\nlisted_in          0\ndescription        0\ndtype: int64\nNumber of duplicate rows: 0\nNumber of duplicate rows: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tData Structuring"
      ],
      "metadata": {
        "id": "jOJaVJiK2t38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date_added' to datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'],format='mixed')\n",
        "# Separate 'duration' into numeric value and unit (e.g., '90 min' â†’ 90, 'min')\n",
        "df[['duration_value', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)')\n",
        "\n",
        "# Convert duration_value to numeric\n",
        "df['duration_value'] = pd.to_numeric(df['duration_value'])\n",
        "\n",
        "# View Resulting columns\n",
        "print(df[['duration_value', 'duration_unit']])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:56:00.374401Z",
          "iopub.execute_input": "2025-06-10T07:56:00.374721Z",
          "iopub.status.idle": "2025-06-10T07:56:00.548981Z",
          "shell.execute_reply.started": "2025-06-10T07:56:00.374692Z",
          "shell.execute_reply": "2025-06-10T07:56:00.548154Z"
        },
        "id": "4OLuQMrD2t3-",
        "outputId": "866cd1cf-064d-40bf-e72b-2170e6260b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "      duration_value duration_unit\n0               90.0           min\n1                2.0       Seasons\n2                1.0        Season\n3                1.0        Season\n4                2.0       Seasons\n...              ...           ...\n8802           158.0           min\n8803             2.0       Seasons\n8804            88.0           min\n8805            88.0           min\n8806           111.0           min\n\n[8807 rows x 2 columns]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "82g8kx2K2t4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows\n",
        "print(\"Duplicate rows before:\", df.duplicated().sum())\n",
        "\n",
        "# Drop duplicate rows if any\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Drop description column because it will not be used\n",
        "df = df.drop(columns=['description'])\n",
        "\n",
        "# Impute Director values by using relationship between cast and director\n",
        "\n",
        "# List of Director-Cast pairs and the number of times they appear\n",
        "df['dir_cast'] = df['director'] + '---' + df['cast']\n",
        "counts = df['dir_cast'].value_counts() #counts unique values\n",
        "filtered_counts = counts[counts >= 3] #checks if repeated 3 or more times\n",
        "filtered_values = filtered_counts.index #gets the values i.e. names\n",
        "lst_dir_cast = list(filtered_values) #convert to list\n",
        "dict_direcast = dict()\n",
        "for i in lst_dir_cast :\n",
        "        director,cast = i.split('---â€™)\n",
        "            dict_direcast[director]=cast\n",
        "for i in range(len(dict_direcast)):\n",
        "    df.loc[(df['director'].isna()) & (df['cast'] == list(dict_direcast.items())[i][1]),'director'] = list(dict_direcast.items())[i][0]\n",
        "\n",
        "# Assign Not Given to all other director fields\n",
        "df.loc[df['director'].isna(),'director'] = 'Not Givenâ€™\n",
        "\n",
        "#Use directors to fill missing countries\n",
        "directors = df['directorâ€™]\n",
        "countries = df['countryâ€™]\n",
        "#pair each director with their country use zip() to get an iterator of tuples\n",
        "pairs = zip(directors, countries)\n",
        "# Convert the list of tuples into a dictionary\n",
        "dir_cntry = dict(list(pairs))\n",
        "\n",
        "# Director matched to Country values used to fill in null country values\n",
        "for i in range(len(dir_cntry)):\n",
        "df.loc[(df['country'].isna()) & (df['director'] == list(dir_cntry.items())[i][0]),'country'] = list(dir_cntry.items())[i][1]\n",
        "# Assign Not Given to all other country fields\n",
        "df.loc[df['country'].isna(),'country'] = 'Not Given'\n",
        "\n",
        "# Assign Not Given to all other fields\n",
        "df.loc[df[â€˜cast'].isna(),â€™cast'] = 'Not Given'\n",
        "\n",
        "# dropping other row records that are null\n",
        "df.drop(df[df['date_added'].isna()].index,axis=0,inplace=True)\n",
        "df.drop(df[df['rating'].isna()].index,axis=0,inplace=True)\n",
        "df.drop(df[df['duration'].isna()].index,axis=0,inplace=True)\n",
        "\n",
        "Errors\n",
        "# check if there are any added_dates that come before release_year\n",
        "import datetime as dt\n",
        "sum(df['date_added'].dt.year < df['release_yearâ€™])\n",
        "df.loc[(df['date_added'].dt.year < df['release_year']),['date_added','release_yearâ€™]]\n",
        "# sample some of the records and check that they have been accurately replaced\n",
        "df.iloc[[1551,1696,2920,3168]]\n",
        "#Confirm that no more release_year inconsistencies\n",
        "sum(df['date_added'].dt.year < df['release_year'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:56:00.551004Z",
          "iopub.execute_input": "2025-06-10T07:56:00.551311Z",
          "iopub.status.idle": "2025-06-10T07:56:00.563785Z",
          "shell.execute_reply.started": "2025-06-10T07:56:00.551288Z",
          "shell.execute_reply": "2025-06-10T07:56:00.562506Z"
        },
        "id": "lFBgiItE2t4G",
        "outputId": "e0b186c1-9302-4ae2-fc64-ec98f1dc8466"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/364497544.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    director,cast = i.split('---â€™)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 20)\n"
          ],
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 20) (364497544.py, line 20)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate\n"
      ],
      "metadata": {
        "id": "plEdaIy52t4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove any columns you may have added during wrangling e.g.\n",
        "df.drop(columns=['dir_cast'], inplace=True)\n",
        "#Check the consistency, accuracy, and completeness of the data\n",
        "#Ensure each column has the correct data type e.g. verify that date_added is datetime and duration_value is numeric.\n",
        "#Use business logic or sanity rules to identify anomalies e.g. records before 1997\n",
        "#Ensure no important fields are still missing\n",
        "#Sample a few rows to check visually e.g.\n",
        "df.sample(5)\n",
        "#Reset the Index e.g.\n",
        "df_reset = df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:56:00.564358Z",
          "iopub.status.idle": "2025-06-10T07:56:00.564806Z",
          "shell.execute_reply.started": "2025-06-10T07:56:00.564671Z",
          "shell.execute_reply": "2025-06-10T07:56:00.564686Z"
        },
        "id": "d64VxhOL2t4L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Publish"
      ],
      "metadata": {
        "id": "fG9NgJ1n2t4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as CSV\n",
        "df.to_csv('/kaggle/working/cleaned_netflix.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-10T07:56:00.56636Z",
          "iopub.status.idle": "2025-06-10T07:56:00.566736Z",
          "shell.execute_reply.started": "2025-06-10T07:56:00.566556Z",
          "shell.execute_reply": "2025-06-10T07:56:00.566574Z"
        },
        "id": "q_2GbJQB2t4O"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}