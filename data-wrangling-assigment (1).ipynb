{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aee57f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T17:10:44.661950Z",
     "iopub.status.busy": "2025-05-26T17:10:44.661522Z",
     "iopub.status.idle": "2025-05-26T17:10:47.103385Z",
     "shell.execute_reply": "2025-05-26T17:10:47.102317Z"
    },
    "papermill": {
     "duration": 2.449501,
     "end_time": "2025-05-26T17:10:47.105125",
     "exception": false,
     "start_time": "2025-05-26T17:10:44.655624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/netflix-shows/netflix_titles.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b7af4",
   "metadata": {
    "papermill": {
     "duration": 0.002705,
     "end_time": "2025-05-26T17:10:47.111275",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.108570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# #Data Science project:Data Wrangling\n",
    "This will demonstrate my walkthrough data wrangling project using python\n",
    "The steps we will go through includes:\n",
    " * Data Discovery\n",
    " * Data Structuring\n",
    "* Data Cleaning\n",
    "* Data Validation\n",
    "* Data Publishing\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ceeaf",
   "metadata": {
    "papermill": {
     "duration": 0.002579,
     "end_time": "2025-05-26T17:10:47.116699",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.114120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1.Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b17b54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T17:10:47.123679Z",
     "iopub.status.busy": "2025-05-26T17:10:47.123232Z",
     "iopub.status.idle": "2025-05-26T17:10:47.358660Z",
     "shell.execute_reply": "2025-05-26T17:10:47.357558Z"
    },
    "papermill": {
     "duration": 0.240857,
     "end_time": "2025-05-26T17:10:47.360310",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.119453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8807 entries, 0 to 8806\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   show_id       8807 non-null   object\n",
      " 1   type          8807 non-null   object\n",
      " 2   title         8807 non-null   object\n",
      " 3   director      6173 non-null   object\n",
      " 4   cast          7982 non-null   object\n",
      " 5   country       7976 non-null   object\n",
      " 6   date_added    8797 non-null   object\n",
      " 7   release_year  8807 non-null   int64 \n",
      " 8   rating        8803 non-null   object\n",
      " 9   duration      8804 non-null   object\n",
      " 10  listed_in     8807 non-null   object\n",
      " 11  description   8807 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 825.8+ KB\n",
      "Shape of the dataset (R x C): (8807, 12)\n",
      "Columns in the dataset:\n",
      " ['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description']\n",
      "Data types:\n",
      " show_id         object\n",
      "type            object\n",
      "title           object\n",
      "director        object\n",
      "cast            object\n",
      "country         object\n",
      "date_added      object\n",
      "release_year     int64\n",
      "rating          object\n",
      "duration        object\n",
      "listed_in       object\n",
      "description     object\n",
      "dtype: object\n",
      "Missing values per column:\n",
      " show_id            0\n",
      "type               0\n",
      "title              0\n",
      "director        2634\n",
      "cast             825\n",
      "country          831\n",
      "date_added        10\n",
      "release_year       0\n",
      "rating             4\n",
      "duration           3\n",
      "listed_in          0\n",
      "description        0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 0\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "#Import the Data to a Pandas DataFrame\n",
    "df=pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\n",
    "#Have a quick overview of the data\n",
    "df.info()\n",
    "# Number of rows and columns\n",
    "print(\"Shape of the dataset (R x C):\", df.shape)\n",
    "# List of all column names\n",
    "print(\"Columns in the dataset:\\n\", df.columns.tolist())\n",
    "# Data types of each column\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "# Group and Count of missing (null) values in each column\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "#Group and Count the  duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "# Group and Count of duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3678ad",
   "metadata": {
    "papermill": {
     "duration": 0.002652,
     "end_time": "2025-05-26T17:10:47.366123",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.363471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2.\tData Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb51ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T17:10:47.373157Z",
     "iopub.status.busy": "2025-05-26T17:10:47.372799Z",
     "iopub.status.idle": "2025-05-26T17:10:47.563956Z",
     "shell.execute_reply": "2025-05-26T17:10:47.562687Z"
    },
    "papermill": {
     "duration": 0.196647,
     "end_time": "2025-05-26T17:10:47.565610",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.368963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      duration_value duration_unit\n",
      "0               90.0           min\n",
      "1                2.0       Seasons\n",
      "2                1.0        Season\n",
      "3                1.0        Season\n",
      "4                2.0       Seasons\n",
      "...              ...           ...\n",
      "8802           158.0           min\n",
      "8803             2.0       Seasons\n",
      "8804            88.0           min\n",
      "8805            88.0           min\n",
      "8806           111.0           min\n",
      "\n",
      "[8807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'date_added' to datetime\n",
    "df['date_added'] = pd.to_datetime(df['date_added'],format='mixed')\n",
    "# Separate 'duration' into numeric value and unit (e.g., '90 min' → 90, 'min')\n",
    "df[['duration_value', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)')\n",
    "\n",
    "# Convert duration_value to numeric\n",
    "df['duration_value'] = pd.to_numeric(df['duration_value'])\n",
    "\n",
    "# View Resulting columns\n",
    "print(df[['duration_value', 'duration_unit']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfd9e7",
   "metadata": {
    "papermill": {
     "duration": 0.002887,
     "end_time": "2025-05-26T17:10:47.571792",
     "exception": false,
     "start_time": "2025-05-26T17:10:47.568905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322ec7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T17:10:47.579460Z",
     "iopub.status.busy": "2025-05-26T17:10:47.578746Z",
     "iopub.status.idle": "2025-05-26T17:10:47.589797Z",
     "shell.execute_reply": "2025-05-26T17:10:47.588511Z"
    },
    "papermill": {
     "duration": 0.016465,
     "end_time": "2025-05-26T17:10:47.591206",
     "exception": true,
     "start_time": "2025-05-26T17:10:47.574741",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 20) (364497544.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_13/364497544.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    director,cast = i.split('---’)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 20)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Duplicate rows before:\", df.duplicated().sum())\n",
    "\n",
    "# Drop duplicate rows if any\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Drop description column because it will not be used\n",
    "df = df.drop(columns=['description'])\n",
    "\n",
    "# Impute Director values by using relationship between cast and director\n",
    "\n",
    "# List of Director-Cast pairs and the number of times they appear\n",
    "df['dir_cast'] = df['director'] + '---' + df['cast']\n",
    "counts = df['dir_cast'].value_counts() #counts unique values\n",
    "filtered_counts = counts[counts >= 3] #checks if repeated 3 or more times\n",
    "filtered_values = filtered_counts.index #gets the values i.e. names\n",
    "lst_dir_cast = list(filtered_values) #convert to list\n",
    "dict_direcast = dict()\n",
    "for i in lst_dir_cast :\n",
    "        director,cast = i.split('---’)\n",
    "            dict_direcast[director]=cast\n",
    "for i in range(len(dict_direcast)): \n",
    "    df.loc[(df['director'].isna()) & (df['cast'] == list(dict_direcast.items())[i][1]),'director'] = list(dict_direcast.items())[i][0]\n",
    "\n",
    "# Assign Not Given to all other director fields\n",
    "df.loc[df['director'].isna(),'director'] = 'Not Given’\n",
    "\n",
    "#Use directors to fill missing countries\n",
    "directors = df['director’]\n",
    "countries = df['country’]\n",
    "#pair each director with their country use zip() to get an iterator of tuples\n",
    "pairs = zip(directors, countries)\n",
    "# Convert the list of tuples into a dictionary\n",
    "dir_cntry = dict(list(pairs))\n",
    "\n",
    "# Director matched to Country values used to fill in null country values\n",
    "for i in range(len(dir_cntry)):    \n",
    "df.loc[(df['country'].isna()) & (df['director'] == list(dir_cntry.items())[i][0]),'country'] = list(dir_cntry.items())[i][1]\n",
    "# Assign Not Given to all other country fields\n",
    "df.loc[df['country'].isna(),'country'] = 'Not Given'\n",
    "\n",
    "# Assign Not Given to all other fields\n",
    "df.loc[df[‘cast'].isna(),’cast'] = 'Not Given'\n",
    "\n",
    "# dropping other row records that are null\n",
    "df.drop(df[df['date_added'].isna()].index,axis=0,inplace=True)\n",
    "df.drop(df[df['rating'].isna()].index,axis=0,inplace=True)\n",
    "df.drop(df[df['duration'].isna()].index,axis=0,inplace=True)\n",
    "\n",
    "Errors\n",
    "# check if there are any added_dates that come before release_year\n",
    "import datetime as dt\n",
    "sum(df['date_added'].dt.year < df['release_year’])\n",
    "df.loc[(df['date_added'].dt.year < df['release_year']),['date_added','release_year’]]\n",
    "# sample some of the records and check that they have been accurately replaced\n",
    "df.iloc[[1551,1696,2920,3168]]\n",
    "#Confirm that no more release_year inconsistencies\n",
    "sum(df['date_added'].dt.year < df['release_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f01afe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0b5b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove any columns you may have added during wrangling e.g.\n",
    "df.drop(columns=['dir_cast'], inplace=True)\n",
    "#Check the consistency, accuracy, and completeness of the data\n",
    "#Ensure each column has the correct data type e.g. verify that date_added is datetime and duration_value is numeric.\n",
    "#Use business logic or sanity rules to identify anomalies e.g. records before 1997\n",
    "#Ensure no important fields are still missing\n",
    "#Sample a few rows to check visually e.g. \n",
    "df.sample(5)\n",
    "#Reset the Index e.g. \n",
    "df_reset = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3aedf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967911d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T17:06:01.009492Z",
     "iopub.status.busy": "2025-05-26T17:06:01.009206Z",
     "iopub.status.idle": "2025-05-26T17:06:01.185112Z",
     "shell.execute_reply": "2025-05-26T17:06:01.184258Z",
     "shell.execute_reply.started": "2025-05-26T17:06:01.009470Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save as CSV \n",
    "df.to_csv('/kaggle/working/cleaned_netflix.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 434238,
     "sourceId": 2654038,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.366714,
   "end_time": "2025-05-26T17:10:48.114040",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-26T17:10:38.747326",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
